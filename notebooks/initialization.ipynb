{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0958fb-5176-4762-a6c4-894e6db1ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80199765-86ad-401f-be6d-8b7707d262dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_in,n_out = 200,126\n",
    "X = np.random.randn(1000,n_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79cc0be-2a0c-442f-8aae-8b659f328af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_der(z):\n",
    "    s = sigmoid(z)\n",
    "    return s(1 - s)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "def tanh_der(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_der(z):\n",
    "    return np.where(z>0,1,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d00d78a-fdf7-467b-b763-8d5fcb95e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Z in Random Wt. Init.: 199.15\n",
      "Variance of A in Random Wt. Init.: 0.22\n"
     ]
    }
   ],
   "source": [
    "#Random Weight Initialization\n",
    "\n",
    "W_rand = np.random.randn(n_in,n_out)\n",
    "Z = X.dot(W_rand)\n",
    "A_rand = sigmoid(Z)\n",
    "\n",
    "print(f\"Variance of Z in Random Wt. Init.: {np.var(Z):.2f}\")\n",
    "print(f\"Variance of A in Random Wt. Init.: {np.var(A_rand):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5ba0b73-92e3-4c2c-b95e-7ebce35ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Z in Xavior Wt. Init.: 1.21\n",
      "Variance of A in Xavior Wt. Init.: 0.05\n"
     ]
    }
   ],
   "source": [
    "#Xavier Weight Initialization\n",
    "\n",
    "W_xav = np.random.randn(n_in,n_out) * np.sqrt(2/(n_in + n_out))\n",
    "Z = X.dot(W_xav)\n",
    "A_xav = sigmoid(Z)\n",
    "\n",
    "print(f\"Variance of Z in Xavior Wt. Init.: {np.var(Z):.2f}\")\n",
    "print(f\"Variance of A in Xavior Wt. Init.: {np.var(A_xav):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7faa9fc8-d292-447b-8c78-1c9f639212b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Z in He Wt. Init.: 2.01\n",
      "Variance of A in He Wt. Init.: 0.68\n"
     ]
    }
   ],
   "source": [
    "#He Weight Initialization\n",
    "\n",
    "W_he = np.random.randn(n_in,n_out) * np.sqrt(2/n_in)\n",
    "Z = X.dot(W_he)\n",
    "A_he = relu(Z)\n",
    "\n",
    "print(f\"Variance of Z in He Wt. Init.: {np.var(Z):.2f}\")\n",
    "print(f\"Variance of A in He Wt. Init.: {np.var(A_he):.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee0e01b7-e0fe-44a6-bfe1-de4df443580d",
   "metadata": {},
   "source": [
    "Now, Above is the pre-activation(Z) and Acitvation(A) Variance of Random,Xavier and He initialization.\n",
    "\n",
    "In Random- weight are distributed through normal distribution around mean = 0 and std = 1. This makes it more prone to activation and gradient saturation like overshooting or shrinking.This happens because every n_in signal will have its variance which will get added which will make-\n",
    "Var(Z) = n_in*var(W)*var(X)\n",
    "with each layer , this will increse with 'to the power' of every layer -> var(Z^L) = n_in^L*var(W)*var(X)\n",
    "\n",
    "Here , in random wt init --> we got var(Z) using sigmoid fn =  200\n",
    "                                    var(A) using sigmoid fn =  0.22\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9453ca9e-a8a9-407f-96e5-751ebe70ef35",
   "metadata": {},
   "source": [
    "Using Wt. init. Techniques like Xavior( for sigmoid and Tanh) and He(for ReLU) can provide a better initialisation of weights and will avoid gradient shrinkage or explosion and can keep them stable across the Layers.\n",
    "We use Xavior for Sigmoid and Tanh because they are symmetric( means there variance is normally centred around 0.sigmoid is largley comsiderd symmetric because its mean is 0.5 which is near 0). He for ReLU and leaky ReLU because it is asymmetric as it grows in positive(in ReLU) and some portion for negative vaues(in Leaky ReLU), which spreads its mean far from 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
